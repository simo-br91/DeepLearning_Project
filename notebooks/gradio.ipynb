{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bef5aae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT = c:\\Users\\mathy\\OneDrive - Groupe INSEEC (POCE)\\Bureau\\Centrale\\Deep Learning\\Deep_Learning_project\n",
      "Device = cpu\n",
      "Loading checkpoint from c:\\Users\\mathy\\OneDrive - Groupe INSEEC (POCE)\\Bureau\\Centrale\\Deep Learning\\Deep_Learning_project\\experiments\\checkpoints\\best_model150epochs.pt\n",
      "Checkpoint chargé (epoch = 135)\n"
     ]
    }
   ],
   "source": [
    "# %% Imports et initialisation\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(\"..\") \n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "from src.utils.config import load_config\n",
    "from src.models.main_cnn import build_model\n",
    "from src.data.datasets import EMOTION_LABELS \n",
    "\n",
    "print(\"PROJECT_ROOT =\", PROJECT_ROOT)\n",
    "\n",
    "# Paths to config and checkpoint\n",
    "CONFIG_PATH = os.path.join(PROJECT_ROOT, \"configs\", \"main_cnn_v1.yaml\")\n",
    "CHECKPOINT_PATH = os.path.join(PROJECT_ROOT, \"experiments\", \"checkpoints\", \"best_model150epochs.pt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device =\", device)\n",
    "\n",
    "# Chargement du config\n",
    "cfg = load_config(CONFIG_PATH)\n",
    "\n",
    "# Construction du modèle exactement comme en training\n",
    "model = build_model(cfg.data).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Chargement des poids (même format que dans main_train.py : \"model_state\")\n",
    "print(f\"Loading checkpoint from {CHECKPOINT_PATH}\")\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location=device, weights_only=False)\n",
    "state_dict = checkpoint[\"model_state\"]\n",
    "model.load_state_dict(state_dict)\n",
    "print(f\"Checkpoint chargé (epoch = {checkpoint.get('epoch', 'unknown')})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6db1b199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Préprocessing et prédiction\n",
    "\n",
    "# Paramètres image issus du YAML\n",
    "IMG_SIZE = int(cfg.data[\"dataset\"].get(\"image_size\", 128))\n",
    "MEAN = np.array([0.5], dtype=np.float32)\n",
    "STD = np.array([0.5], dtype=np.float32)\n",
    "\n",
    "def preprocess_image(pil_img: Image.Image) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    - Convertit en niveaux de gris\n",
    "    - Resize à IMG_SIZE x IMG_SIZE\n",
    "    - Normalise comme en training\n",
    "    - Retourne un tenseur (1, 1, H, W) sur le device\n",
    "    \"\"\"\n",
    "    # Conversion en niveaux de gris (1 canal)\n",
    "    img = pil_img.convert(\"L\")\n",
    "\n",
    "    # Resize\n",
    "    img = img.resize((IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    # -> np.array [H, W], puis normalisation [0,1]\n",
    "    arr = np.array(img).astype(\"float32\") / 255.0  # (H, W)\n",
    "\n",
    "    # Normalisation (x - mean)/std\n",
    "    arr = (arr - MEAN[0]) / STD[0]\n",
    "\n",
    "    # Ajout des dimensions channel + batch : (1, 1, H, W)\n",
    "    tensor = torch.from_numpy(arr).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Envoi sur device\n",
    "    tensor = tensor.to(device)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def predict_emotion(pil_img: Image.Image):\n",
    "    \"\"\"\n",
    "    Fonction appelée par Gradio :\n",
    "    renvoie l'étiquette d'émotion + confiance (%).\n",
    "    \"\"\"\n",
    "    if pil_img is None:\n",
    "        return \"No image provided.\"\n",
    "\n",
    "    x = preprocess_image(pil_img)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)\n",
    "        probs = torch.softmax(logits, dim=1)[0].cpu().numpy()\n",
    "\n",
    "    top_idx = int(probs.argmax())\n",
    "    top_label = EMOTION_LABELS[top_idx]\n",
    "    top_conf = float(probs[top_idx])\n",
    "\n",
    "    # Optionnel : construire un dict label -> prob pour debug\n",
    "    # proba_dict = {label: float(p) for label, p in zip(EMOTION_LABELS, probs)}\n",
    "\n",
    "    return f\"Predicted emotion : {top_label} ({top_conf * 100:.2f} % confidence)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3bbbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://73c2b8a7c22f25ed2b.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://73c2b8a7c22f25ed2b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% Interface Gradio\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=predict_emotion,\n",
    "    inputs=gr.Image(type=\"pil\", label=\"Input Image\"),\n",
    "    outputs=gr.Textbox(label=\"Prediction\"),\n",
    "    title=\"Emotion Recognition\",\n",
    "    description=(\n",
    "        \"Load an image of a face.\"\n",
    "        \"predicts the dominant emotion and the associated confidence.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "iface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
