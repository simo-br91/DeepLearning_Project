{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eeafff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final results & analysis notebook\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    sns.set(style=\"whitegrid\")\n",
    "except ImportError:\n",
    "    sns = None\n",
    "    print(\"Seaborn non installé, les heatmaps seront moins jolies.\")\n",
    "\n",
    "# Répertoires principaux\n",
    "PROJECT_ROOT = os.path.abspath(\"..\")  # si le notebook est dans notebooks/\n",
    "EVAL_DIR = os.path.join(PROJECT_ROOT, \"experiments\", \"evaluation\")\n",
    "\n",
    "print(\"PROJECT_ROOT =\", PROJECT_ROOT)\n",
    "print(\"EVAL_DIR     =\", EVAL_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab6b050",
   "metadata": {},
   "source": [
    "## Load Metric Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b68937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_path = os.path.join(EVAL_DIR, \"test_overall_metrics.csv\")\n",
    "per_class_path = os.path.join(EVAL_DIR, \"test_per_class_metrics.csv\")\n",
    "conf_counts_path = os.path.join(EVAL_DIR, \"test_confusion_counts.csv\")\n",
    "conf_norm_path = os.path.join(EVAL_DIR, \"test_confusion_normalized.csv\")\n",
    "preds_path = os.path.join(EVAL_DIR, \"test_predictions.csv\")\n",
    "\n",
    "print(\"Existence des fichiers :\")\n",
    "for p in [overall_path, per_class_path, conf_counts_path, conf_norm_path, preds_path]:\n",
    "    print(os.path.basename(p), \"→\", os.path.exists(p))\n",
    "\n",
    "overall_df = pd.read_csv(overall_path) if os.path.exists(overall_path) else None\n",
    "per_class_df = pd.read_csv(per_class_path) if os.path.exists(per_class_path) else None\n",
    "conf_counts_df = pd.read_csv(conf_counts_path, index_col=0) if os.path.exists(conf_counts_path) else None\n",
    "conf_norm_df = pd.read_csv(conf_norm_path, index_col=0) if os.path.exists(conf_norm_path) else None\n",
    "preds_df = pd.read_csv(preds_path) if os.path.exists(preds_path) else None\n",
    "\n",
    "overall_df, per_class_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de479ec",
   "metadata": {},
   "source": [
    "## Display Overall Test Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794d3423",
   "metadata": {},
   "outputs": [],
   "source": [
    "if overall_df is None:\n",
    "    print(\"Pas de test_overall_metrics.csv trouvé.\")\n",
    "else:\n",
    "    print(\"=== Métriques globales (test set) ===\")\n",
    "    display(overall_df)\n",
    "\n",
    "    # Si les colonnes existent\n",
    "    cols = [c for c in [\"accuracy\", \"balanced_accuracy\", \"macro_f1\", \"weighted_f1\"] if c in overall_df.columns]\n",
    "    if cols:\n",
    "        plt.figure()\n",
    "        overall_df[cols].iloc[0].plot(kind=\"bar\")\n",
    "        plt.title(\"Métriques globales (test)\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.ylim(0, 1)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7154b1",
   "metadata": {},
   "source": [
    "## Per-Class Metrics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8140b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if per_class_df is None:\n",
    "    print(\"Pas de test_per_class_metrics.csv trouvé.\")\n",
    "else:\n",
    "    print(\"=== Métriques par classe ===\")\n",
    "    display(per_class_df)\n",
    "\n",
    "    # Bar plot par classe pour F1\n",
    "    if \"f1\" in per_class_df.columns:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.bar(per_class_df[\"class\"], per_class_df[\"f1\"])\n",
    "        plt.title(\"F1-score par classe\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylim(0, 1)\n",
    "        plt.ylabel(\"F1-score\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Optionnel : precision / recall\n",
    "    for metric in [\"precision\", \"recall\"]:\n",
    "        if metric in per_class_df.columns:\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            plt.bar(per_class_df[\"class\"], per_class_df[metric])\n",
    "            plt.title(f\"{metric.capitalize()} par classe\")\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.ylim(0, 1)\n",
    "            plt.ylabel(metric.capitalize())\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c875dd7",
   "metadata": {},
   "source": [
    "## Confusion Matrices (Counts & Normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db44c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(df_conf, title, cmap=\"Blues\"):\n",
    "    if df_conf is None:\n",
    "        print(f\"{title}: matrice non disponible.\")\n",
    "        return\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    if sns is not None:\n",
    "        sns.heatmap(df_conf, annot=True, fmt=\".2f\" if \"normalized\" in title.lower() else \"d\",\n",
    "                    cmap=cmap, cbar=True)\n",
    "    else:\n",
    "        plt.imshow(df_conf.values, cmap=cmap)\n",
    "        for i in range(df_conf.shape[0]):\n",
    "            for j in range(df_conf.shape[1]):\n",
    "                plt.text(j, i, f\"{df_conf.values[i, j]:.2f}\", ha=\"center\", va=\"center\", color=\"black\")\n",
    "        plt.colorbar()\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xticks(np.arange(len(df_conf.columns)) + 0.5, df_conf.columns, rotation=45)\n",
    "    plt.yticks(np.arange(len(df_conf.index)) + 0.5, df_conf.index, rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_confusion_matrix(conf_counts_df, \"Confusion matrix (counts)\")\n",
    "plot_confusion_matrix(conf_norm_df, \"Confusion matrix (normalized)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f181bb",
   "metadata": {},
   "source": [
    "## Error Analysis (Most Confused Pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9076f598",
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf_norm_df is not None:\n",
    "    errors = conf_norm_df.copy()\n",
    "    np.fill_diagonal(errors.values, 0.0)\n",
    "\n",
    "    # on \"flatten\" les erreurs\n",
    "    error_list = []\n",
    "    for i, true_class in enumerate(errors.index):\n",
    "        for j, pred_class in enumerate(errors.columns):\n",
    "            error_list.append({\n",
    "                \"true\": true_class,\n",
    "                \"pred\": pred_class,\n",
    "                \"rate\": errors.values[i, j]\n",
    "            })\n",
    "    errors_df = pd.DataFrame(error_list)\n",
    "    errors_df = errors_df.sort_values(\"rate\", ascending=False)\n",
    "\n",
    "    print(\"Top 10 des paires les plus confondues (taux normalisé) :\")\n",
    "    display(errors_df.head(10))\n",
    "else:\n",
    "    print(\"Pas de matrice de confusion normalisée pour analyser les erreurs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4585a3f",
   "metadata": {},
   "source": [
    "## Prediction-Level Analysis (Correct vs Incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d11ba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "if preds_df is not None:\n",
    "    print(\"Aperçu des prédictions :\")\n",
    "    display(preds_df.head())\n",
    "\n",
    "    if \"true_label\" in preds_df.columns and \"pred_label\" in preds_df.columns:\n",
    "        preds_df[\"correct\"] = preds_df[\"true_label\"] == preds_df[\"pred_label\"]\n",
    "        acc = preds_df[\"correct\"].mean()\n",
    "        print(f\"Accuracy recalculée depuis predictions.csv : {acc:.4f}\")\n",
    "\n",
    "        print(\"\\nExemples mal classés :\")\n",
    "        display(preds_df[~preds_df[\"correct\"]].head(10))\n",
    "else:\n",
    "    print(\"Pas de test_predictions.csv pour analyser au niveau exemple.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
