2025-11-24 11:18:34,930 | root | INFO | Logging initialized. Writing to: experiments\logs\train.log
2025-11-24 11:18:34,935 | train | INFO | Loaded config from: configs/main_cnn_template.yaml
2025-11-24 11:18:34,936 | train | INFO | Config content:
Config(path=WindowsPath('configs/main_cnn_template.yaml'), data={'seed': 42, 'dataset': {'name': 'RAFDB', 'root': 'data/processed', 'splits_dir': 'data/splits', 'image_size': 128, 'channels': 1, 'train_split': 'train.csv', 'val_split': 'val.csv', 'test_split': 'test.csv'}, 'preprocessing': {'normalize': {'mean': [0.5], 'std': [0.5]}, 'use_hist_equalization': False}, 'augmentation': {'horizontal_flip': True, 'rotation_deg': 15, 'translate': 0.1, 'brightness': 0.2, 'contrast': 0.2, 'saturation': 0.0, 'hue': 0.0, 'random_erasing': True, 'random_erasing_p': 0.5, 'random_erasing_scale': [0.02, 0.33], 'random_erasing_ratio': [0.3, 3.3]}, 'training': {'batch_size': 64, 'num_epochs': 20, 'num_workers': 4, 'pin_memory': True, 'early_stopping_patience': 15}, 'optimizer': {'name': 'adamw', 'lr': 0.001, 'weight_decay': 0.0001, 'momentum': 0.9}, 'scheduler': {'name': 'cosine_annealing', 'T_max': 50, 'eta_min': 1e-05}, 'logging': {'log_dir': 'experiments/logs', 'checkpoint_dir': 'experiments/checkpoints', 'save_best_metric': 'val_macro_f1', 'tensorboard': True}})
2025-11-24 11:18:34,937 | src.utils.seed | INFO | Setting global seed to 42
2025-11-24 11:18:34,945 | src.utils.seed | INFO | PyTorch cuDNN set to deterministic mode
2025-11-24 11:18:34,945 | train | INFO | Seed set to 42
2025-11-24 11:18:34,946 | train | INFO | Using device: cuda
2025-11-24 11:18:36,787 | train | INFO | Computing class counts for training set...
2025-11-24 11:19:13,363 | train | INFO | Class counts: [4338, 6292, 4254, 2801, 3585, 383, 3467]
2025-11-24 11:19:13,461 | train | INFO | Class weights (inverse frequency): [0.8272410035133362, 0.5703387260437012, 0.8435757756233215, 1.281175136566162, 1.0009962320327759, 9.369638442993164, 1.0350652933120728]
2025-11-24 11:19:13,506 | train | INFO | Model built:
MainCNN(
  (features): Sequential(
    (0): ConvBlock(
      (conv1): DepthwiseSeparableConv2d(
        (depthwise): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (pointwise): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ELU(alpha=1.0, inplace=True)
      (conv2): DepthwiseSeparableConv2d(
        (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ELU(alpha=1.0, inplace=True)
      (res_proj): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (dropout): Dropout2d(p=0.25, inplace=False)
    )
    (1): ConvBlock(
      (conv1): DepthwiseSeparableConv2d(
        (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (pointwise): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ELU(alpha=1.0, inplace=True)
      (conv2): DepthwiseSeparableConv2d(
        (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
        (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ELU(alpha=1.0, inplace=True)
      (res_proj): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (attention): SEBlock(
        (avg_pool): AdaptiveAvgPool2d(output_size=1)
        (fc): Sequential(
          (0): Linear(in_features=64, out_features=4, bias=False)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=4, out_features=64, bias=False)
          (3): Sigmoid()
        )
      )
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (dropout): Dropout2d(p=0.25, inplace=False)
    )
    (2): ConvBlock(
      (conv1): DepthwiseSeparableConv2d(
        (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
        (pointwise): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ELU(alpha=1.0, inplace=True)
      (conv2): DepthwiseSeparableConv2d(
        (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
        (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ELU(alpha=1.0, inplace=True)
      (res_proj): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (attention): SEBlock(
        (avg_pool): AdaptiveAvgPool2d(output_size=1)
        (fc): Sequential(
          (0): Linear(in_features=128, out_features=8, bias=False)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=8, out_features=128, bias=False)
          (3): Sigmoid()
        )
      )
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (dropout): Dropout2d(p=0.25, inplace=False)
    )
    (3): ConvBlock(
      (conv1): DepthwiseSeparableConv2d(
        (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
        (pointwise): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ELU(alpha=1.0, inplace=True)
      (conv2): DepthwiseSeparableConv2d(
        (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
        (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ELU(alpha=1.0, inplace=True)
      (res_proj): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (attention): SEBlock(
        (avg_pool): AdaptiveAvgPool2d(output_size=1)
        (fc): Sequential(
          (0): Linear(in_features=256, out_features=16, bias=False)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=16, out_features=256, bias=False)
          (3): Sigmoid()
        )
      )
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (dropout): Dropout2d(p=0.25, inplace=False)
    )
  )
  (gap): AdaptiveAvgPool2d(output_size=1)
  (classifier): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Linear(in_features=256, out_features=256, bias=True)
    (2): ELU(alpha=1.0, inplace=True)
    (3): Dropout(p=0.4, inplace=False)
    (4): Linear(in_features=256, out_features=7, bias=True)
  )
)
2025-11-24 11:19:13,507 | train | INFO | Using AdamW optimizer (lr=0.001, weight_decay=0.0001)
2025-11-24 11:19:13,508 | train | INFO | Using CosineAnnealingLR scheduler (T_max=50, eta_min=1e-05)
2025-11-24 11:19:13,512 | train | INFO | Starting training for 20 epochs. Early stopping patience = 15, best metric = val_macro_f1
2025-11-24 11:21:02,043 | train | INFO | Epoch 001/020 [108.5s] Train: loss=2.1548, acc=0.1588, macroF1=0.1479 | Val: loss=1.9363, acc=0.1523, macroF1=0.0946
2025-11-24 11:21:02,044 | train | INFO | New best model at epoch 1 (val_macro_f1=0.0946). Saving checkpoint to experiments\checkpoints\best_model.pt
2025-11-24 11:22:06,745 | train | INFO | Epoch 002/020 [64.7s] Train: loss=1.9981, acc=0.1580, macroF1=0.1484 | Val: loss=1.9353, acc=0.1748, macroF1=0.0794
2025-11-24 11:22:06,746 | train | INFO | No improvement in best metric for 1 epoch(s). Best so far: 0.0946 at epoch 1
2025-11-24 11:23:12,531 | train | INFO | Epoch 003/020 [65.8s] Train: loss=1.9619, acc=0.1621, macroF1=0.1522 | Val: loss=1.9574, acc=0.1090, macroF1=0.0891
2025-11-24 11:23:12,532 | train | INFO | No improvement in best metric for 2 epoch(s). Best so far: 0.0946 at epoch 1
2025-11-24 11:24:17,323 | train | INFO | Epoch 004/020 [64.8s] Train: loss=1.9466, acc=0.1652, macroF1=0.1527 | Val: loss=1.9253, acc=0.1887, macroF1=0.1329
2025-11-24 11:24:17,323 | train | INFO | New best model at epoch 4 (val_macro_f1=0.1329). Saving checkpoint to experiments\checkpoints\best_model.pt
2025-11-24 11:25:21,956 | train | INFO | Epoch 005/020 [64.6s] Train: loss=1.9380, acc=0.1702, macroF1=0.1546 | Val: loss=1.9259, acc=0.1795, macroF1=0.1038
2025-11-24 11:25:21,957 | train | INFO | No improvement in best metric for 1 epoch(s). Best so far: 0.1329 at epoch 4
2025-11-24 11:26:26,053 | train | INFO | Epoch 006/020 [64.1s] Train: loss=1.9343, acc=0.1715, macroF1=0.1534 | Val: loss=1.9183, acc=0.1982, macroF1=0.1400
2025-11-24 11:26:26,054 | train | INFO | New best model at epoch 6 (val_macro_f1=0.1400). Saving checkpoint to experiments\checkpoints\best_model.pt
2025-11-24 11:27:30,438 | train | INFO | Epoch 007/020 [64.4s] Train: loss=1.9354, acc=0.1637, macroF1=0.1463 | Val: loss=1.9247, acc=0.1845, macroF1=0.1140
2025-11-24 11:27:30,438 | train | INFO | No improvement in best metric for 1 epoch(s). Best so far: 0.1400 at epoch 6
2025-11-24 11:28:34,609 | train | INFO | Epoch 008/020 [64.2s] Train: loss=1.9319, acc=0.1716, macroF1=0.1529 | Val: loss=1.9236, acc=0.1581, macroF1=0.1342
2025-11-24 11:28:34,610 | train | INFO | No improvement in best metric for 2 epoch(s). Best so far: 0.1400 at epoch 6
2025-11-24 11:29:39,031 | train | INFO | Epoch 009/020 [64.4s] Train: loss=1.9306, acc=0.1791, macroF1=0.1574 | Val: loss=1.9210, acc=0.1941, macroF1=0.1543
2025-11-24 11:29:39,032 | train | INFO | New best model at epoch 9 (val_macro_f1=0.1543). Saving checkpoint to experiments\checkpoints\best_model.pt
2025-11-24 11:30:43,798 | train | INFO | Epoch 010/020 [64.7s] Train: loss=1.9263, acc=0.1742, macroF1=0.1550 | Val: loss=1.9141, acc=0.2278, macroF1=0.1415
2025-11-24 11:30:43,798 | train | INFO | No improvement in best metric for 1 epoch(s). Best so far: 0.1543 at epoch 9
2025-11-24 11:31:48,735 | train | INFO | Epoch 011/020 [64.9s] Train: loss=1.9255, acc=0.1778, macroF1=0.1615 | Val: loss=1.9126, acc=0.2296, macroF1=0.1499
2025-11-24 11:31:48,735 | train | INFO | No improvement in best metric for 2 epoch(s). Best so far: 0.1543 at epoch 9
2025-11-24 11:32:54,917 | train | INFO | Epoch 012/020 [66.2s] Train: loss=1.9253, acc=0.1758, macroF1=0.1564 | Val: loss=1.9085, acc=0.2025, macroF1=0.1377
2025-11-24 11:32:54,918 | train | INFO | No improvement in best metric for 3 epoch(s). Best so far: 0.1543 at epoch 9
2025-11-24 11:34:00,112 | train | INFO | Epoch 013/020 [65.2s] Train: loss=1.9211, acc=0.1766, macroF1=0.1545 | Val: loss=1.9259, acc=0.1195, macroF1=0.1003
2025-11-24 11:34:00,112 | train | INFO | No improvement in best metric for 4 epoch(s). Best so far: 0.1543 at epoch 9
2025-11-24 11:35:05,307 | train | INFO | Epoch 014/020 [65.2s] Train: loss=1.9208, acc=0.1888, macroF1=0.1629 | Val: loss=1.9094, acc=0.2395, macroF1=0.1605
2025-11-24 11:35:05,308 | train | INFO | New best model at epoch 14 (val_macro_f1=0.1605). Saving checkpoint to experiments\checkpoints\best_model.pt
2025-11-24 11:36:10,070 | train | INFO | Epoch 015/020 [64.7s] Train: loss=1.9198, acc=0.1965, macroF1=0.1691 | Val: loss=1.9083, acc=0.2082, macroF1=0.1629
2025-11-24 11:36:10,071 | train | INFO | New best model at epoch 15 (val_macro_f1=0.1629). Saving checkpoint to experiments\checkpoints\best_model.pt
2025-11-24 11:37:14,455 | train | INFO | Epoch 016/020 [64.4s] Train: loss=1.9175, acc=0.1851, macroF1=0.1627 | Val: loss=1.9039, acc=0.2029, macroF1=0.1491
2025-11-24 11:37:14,455 | train | INFO | No improvement in best metric for 1 epoch(s). Best so far: 0.1629 at epoch 15
2025-11-24 11:38:19,032 | train | INFO | Epoch 017/020 [64.6s] Train: loss=1.9159, acc=0.1811, macroF1=0.1589 | Val: loss=1.9147, acc=0.1466, macroF1=0.1257
2025-11-24 11:38:19,032 | train | INFO | No improvement in best metric for 2 epoch(s). Best so far: 0.1629 at epoch 15
2025-11-24 11:39:23,850 | train | INFO | Epoch 018/020 [64.8s] Train: loss=1.9143, acc=0.1916, macroF1=0.1665 | Val: loss=1.9154, acc=0.1256, macroF1=0.1201
2025-11-24 11:39:23,851 | train | INFO | No improvement in best metric for 3 epoch(s). Best so far: 0.1629 at epoch 15
2025-11-24 11:40:29,208 | train | INFO | Epoch 019/020 [65.4s] Train: loss=1.9150, acc=0.1861, macroF1=0.1627 | Val: loss=1.8963, acc=0.1469, macroF1=0.1224
2025-11-24 11:40:29,209 | train | INFO | No improvement in best metric for 4 epoch(s). Best so far: 0.1629 at epoch 15
2025-11-24 11:41:34,599 | train | INFO | Epoch 020/020 [65.4s] Train: loss=1.9149, acc=0.1900, macroF1=0.1645 | Val: loss=1.9173, acc=0.1560, macroF1=0.1407
2025-11-24 11:41:34,600 | train | INFO | No improvement in best metric for 5 epoch(s). Best so far: 0.1629 at epoch 15
