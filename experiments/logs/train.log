2025-11-23 22:42:01,378 | root | INFO | Logging initialized. Writing to: experiments\logs\train.log
2025-11-23 22:42:01,383 | train | INFO | Loaded config from: configs/main_cnn_template.yaml
2025-11-23 22:42:01,383 | train | INFO | Config content:
Config(path=WindowsPath('configs/main_cnn_template.yaml'), data={'seed': 42, 'dataset': {'name': 'RAFDB', 'root': 'data/processed', 'splits_dir': 'data/splits', 'image_size': 128, 'channels': 1, 'train_split': 'train.csv', 'val_split': 'val.csv', 'test_split': 'test.csv'}, 'preprocessing': {'normalize': {'mean': [0.5], 'std': [0.5]}, 'use_hist_equalization': False}, 'augmentation': {'horizontal_flip': True, 'rotation_deg': 15, 'translate': 0.1, 'brightness': 0.2, 'contrast': 0.2, 'saturation': 0.0, 'hue': 0.0, 'random_erasing': True, 'random_erasing_p': 0.5, 'random_erasing_scale': [0.02, 0.33], 'random_erasing_ratio': [0.3, 3.3]}, 'training': {'batch_size': 64, 'num_epochs': 150, 'num_workers': 4, 'pin_memory': True, 'early_stopping_patience': 15}, 'optimizer': {'name': 'adamw', 'lr': 0.001, 'weight_decay': 0.0001, 'momentum': 0.9}, 'scheduler': {'name': 'cosine_annealing', 'T_max': 50, 'eta_min': 1e-05}, 'logging': {'log_dir': 'experiments/logs', 'checkpoint_dir': 'experiments/checkpoints', 'save_best_metric': 'val_macro_f1', 'tensorboard': True}})
2025-11-23 22:42:01,383 | src.utils.seed | INFO | Setting global seed to 42
2025-11-23 22:42:01,388 | src.utils.seed | INFO | PyTorch cuDNN set to deterministic mode
2025-11-23 22:42:01,388 | train | INFO | Seed set to 42
2025-11-23 22:42:01,388 | train | INFO | Using device: cuda
2025-11-23 22:42:02,500 | train | INFO | Computing class counts for training set...
2025-11-23 22:42:34,073 | train | INFO | Class counts: [4338, 6292, 4254, 2801, 3585, 383, 3467]
2025-11-23 22:42:34,078 | train | INFO | Class weights (inverse frequency): [0.8272410035133362, 0.5703387260437012, 0.8435757756233215, 1.281175136566162, 1.0009962320327759, 9.369638442993164, 1.0350652933120728]
2025-11-23 22:42:34,096 | train | INFO | Model built:
MainCNN(
  (features): Sequential(
    (0): ConvBlock(
      (conv1): DepthwiseSeparableConv2d(
        (depthwise): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (pointwise): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ELU(alpha=1.0, inplace=True)
      (conv2): DepthwiseSeparableConv2d(
        (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ELU(alpha=1.0, inplace=True)
      (res_proj): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (dropout): Dropout2d(p=0.25, inplace=False)
    )
    (1): ConvBlock(
      (conv1): DepthwiseSeparableConv2d(
        (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (pointwise): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ELU(alpha=1.0, inplace=True)
      (conv2): DepthwiseSeparableConv2d(
        (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
        (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ELU(alpha=1.0, inplace=True)
      (res_proj): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (attention): SEBlock(
        (avg_pool): AdaptiveAvgPool2d(output_size=1)
        (fc): Sequential(
          (0): Linear(in_features=64, out_features=4, bias=False)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=4, out_features=64, bias=False)
          (3): Sigmoid()
        )
      )
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (dropout): Dropout2d(p=0.25, inplace=False)
    )
    (2): ConvBlock(
      (conv1): DepthwiseSeparableConv2d(
        (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
        (pointwise): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ELU(alpha=1.0, inplace=True)
      (conv2): DepthwiseSeparableConv2d(
        (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
        (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ELU(alpha=1.0, inplace=True)
      (res_proj): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (attention): SEBlock(
        (avg_pool): AdaptiveAvgPool2d(output_size=1)
        (fc): Sequential(
          (0): Linear(in_features=128, out_features=8, bias=False)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=8, out_features=128, bias=False)
          (3): Sigmoid()
        )
      )
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (dropout): Dropout2d(p=0.25, inplace=False)
    )
    (3): ConvBlock(
      (conv1): DepthwiseSeparableConv2d(
        (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
        (pointwise): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ELU(alpha=1.0, inplace=True)
      (conv2): DepthwiseSeparableConv2d(
        (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
        (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ELU(alpha=1.0, inplace=True)
      (res_proj): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (attention): SEBlock(
        (avg_pool): AdaptiveAvgPool2d(output_size=1)
        (fc): Sequential(
          (0): Linear(in_features=256, out_features=16, bias=False)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=16, out_features=256, bias=False)
          (3): Sigmoid()
        )
      )
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (dropout): Dropout2d(p=0.25, inplace=False)
    )
  )
  (gap): AdaptiveAvgPool2d(output_size=1)
  (classifier): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Linear(in_features=256, out_features=256, bias=True)
    (2): ELU(alpha=1.0, inplace=True)
    (3): Dropout(p=0.4, inplace=False)
    (4): Linear(in_features=256, out_features=7, bias=True)
  )
)
2025-11-23 22:42:34,096 | train | INFO | Using AdamW optimizer (lr=0.001, weight_decay=0.0001)
2025-11-23 22:42:34,097 | train | INFO | Using CosineAnnealingLR scheduler (T_max=50, eta_min=1e-05)
2025-11-23 22:42:34,100 | train | INFO | Starting training for 150 epochs. Early stopping patience = 15, best metric = val_macro_f1
2025-11-23 22:43:44,419 | train | INFO | Epoch 001/150 [70.3s] Train: loss=2.1548, acc=0.1588, macroF1=0.1479 | Val: loss=1.9363, acc=0.1523, macroF1=0.0946
2025-11-23 22:43:44,419 | train | INFO | New best model at epoch 1 (val_macro_f1=0.0946). Saving checkpoint to experiments\checkpoints\best_model.pt
2025-11-23 22:44:49,777 | train | INFO | Epoch 002/150 [65.3s] Train: loss=1.9981, acc=0.1580, macroF1=0.1484 | Val: loss=1.9353, acc=0.1748, macroF1=0.0794
2025-11-23 22:44:49,779 | train | INFO | No improvement in best metric for 1 epoch(s). Best so far: 0.0946 at epoch 1
2025-11-23 22:46:02,813 | train | INFO | Epoch 003/150 [73.0s] Train: loss=1.9619, acc=0.1621, macroF1=0.1522 | Val: loss=1.9574, acc=0.1090, macroF1=0.0891
2025-11-23 22:46:02,816 | train | INFO | No improvement in best metric for 2 epoch(s). Best so far: 0.0946 at epoch 1
2025-11-23 22:47:12,107 | train | INFO | Epoch 004/150 [69.3s] Train: loss=1.9466, acc=0.1652, macroF1=0.1527 | Val: loss=1.9253, acc=0.1887, macroF1=0.1329
2025-11-23 22:47:12,108 | train | INFO | New best model at epoch 4 (val_macro_f1=0.1329). Saving checkpoint to experiments\checkpoints\best_model.pt
2025-11-23 22:48:19,889 | train | INFO | Epoch 005/150 [67.8s] Train: loss=1.9380, acc=0.1702, macroF1=0.1546 | Val: loss=1.9259, acc=0.1795, macroF1=0.1038
2025-11-23 22:48:19,891 | train | INFO | No improvement in best metric for 1 epoch(s). Best so far: 0.1329 at epoch 4
2025-11-23 22:49:28,677 | train | INFO | Epoch 006/150 [68.8s] Train: loss=1.9343, acc=0.1715, macroF1=0.1534 | Val: loss=1.9183, acc=0.1982, macroF1=0.1400
2025-11-23 22:49:28,677 | train | INFO | New best model at epoch 6 (val_macro_f1=0.1400). Saving checkpoint to experiments\checkpoints\best_model.pt
2025-11-23 22:50:36,241 | train | INFO | Epoch 007/150 [67.5s] Train: loss=1.9354, acc=0.1637, macroF1=0.1463 | Val: loss=1.9247, acc=0.1845, macroF1=0.1140
2025-11-23 22:50:36,241 | train | INFO | No improvement in best metric for 1 epoch(s). Best so far: 0.1400 at epoch 6
2025-11-23 22:51:42,483 | train | INFO | Epoch 008/150 [66.2s] Train: loss=1.9319, acc=0.1716, macroF1=0.1529 | Val: loss=1.9236, acc=0.1581, macroF1=0.1342
2025-11-23 22:51:42,483 | train | INFO | No improvement in best metric for 2 epoch(s). Best so far: 0.1400 at epoch 6
2025-11-23 22:52:51,024 | train | INFO | Epoch 009/150 [68.5s] Train: loss=1.9306, acc=0.1791, macroF1=0.1574 | Val: loss=1.9210, acc=0.1941, macroF1=0.1543
2025-11-23 22:52:51,024 | train | INFO | New best model at epoch 9 (val_macro_f1=0.1543). Saving checkpoint to experiments\checkpoints\best_model.pt
2025-11-23 22:54:00,154 | train | INFO | Epoch 010/150 [69.1s] Train: loss=1.9263, acc=0.1742, macroF1=0.1550 | Val: loss=1.9141, acc=0.2278, macroF1=0.1415
2025-11-23 22:54:00,155 | train | INFO | No improvement in best metric for 1 epoch(s). Best so far: 0.1543 at epoch 9
2025-11-23 22:55:07,814 | train | INFO | Epoch 011/150 [67.7s] Train: loss=1.9255, acc=0.1778, macroF1=0.1615 | Val: loss=1.9126, acc=0.2296, macroF1=0.1499
2025-11-23 22:55:07,814 | train | INFO | No improvement in best metric for 2 epoch(s). Best so far: 0.1543 at epoch 9
2025-11-23 22:56:14,164 | train | INFO | Epoch 012/150 [66.3s] Train: loss=1.9253, acc=0.1758, macroF1=0.1564 | Val: loss=1.9085, acc=0.2025, macroF1=0.1377
2025-11-23 22:56:14,165 | train | INFO | No improvement in best metric for 3 epoch(s). Best so far: 0.1543 at epoch 9
2025-11-23 22:57:28,369 | train | INFO | Epoch 013/150 [74.2s] Train: loss=1.9211, acc=0.1766, macroF1=0.1545 | Val: loss=1.9259, acc=0.1195, macroF1=0.1003
2025-11-23 22:57:28,372 | train | INFO | No improvement in best metric for 4 epoch(s). Best so far: 0.1543 at epoch 9
2025-11-23 22:58:36,014 | train | INFO | Epoch 014/150 [67.6s] Train: loss=1.9208, acc=0.1888, macroF1=0.1629 | Val: loss=1.9094, acc=0.2395, macroF1=0.1605
2025-11-23 22:58:36,015 | train | INFO | New best model at epoch 14 (val_macro_f1=0.1605). Saving checkpoint to experiments\checkpoints\best_model.pt
2025-11-23 22:59:42,641 | train | INFO | Epoch 015/150 [66.6s] Train: loss=1.9198, acc=0.1965, macroF1=0.1691 | Val: loss=1.9083, acc=0.2082, macroF1=0.1629
2025-11-23 22:59:42,641 | train | INFO | New best model at epoch 15 (val_macro_f1=0.1629). Saving checkpoint to experiments\checkpoints\best_model.pt
